# Google Colab TIDE-Lite Training Commands
# Copy these commands into Colab cells

# ============================================
# SETUP COMMANDS (Run in order)
# ============================================

# Cell 1: Enable GPU
# Runtime -> Change runtime type -> Hardware accelerator: GPU (T4 or better)

# Cell 2: Check GPU and install dependencies
!nvidia-smi
!pip install -q transformers datasets scipy scikit-learn tqdm matplotlib seaborn sentence-transformers tensorboard

# Cell 3: Clone repository
!git clone https://github.com/[YOUR_USERNAME]/DynamicEmbeddings.git
%cd DynamicEmbeddings

# Cell 4: Quick setup check
import torch
print(f"✅ PyTorch: {torch.__version__}")
print(f"✅ CUDA: {torch.cuda.is_available()}")
print(f"✅ GPU: {torch.cuda.get_device_name(0)}")

# ============================================
# TRAINING COMMANDS
# ============================================

# Cell 5: Create optimized Colab config
%%writefile configs/colab_gpu.yaml
encoder_name: "sentence-transformers/all-MiniLM-L6-v2"
hidden_dim: 384
time_encoding_dim: 64
mlp_hidden_dim: 256
mlp_dropout: 0.15
freeze_encoder: true
pooling_strategy: "mean"
gate_activation: "sigmoid"
batch_size: 256
eval_batch_size: 512
max_seq_length: 128
num_workers: 2
num_epochs: 20
learning_rate: 2.0e-5
warmup_steps: 500
weight_decay: 0.01
gradient_clip: 1.0
temporal_weight: 0.15
preservation_weight: 0.03
tau_seconds: 86400.0
use_amp: true
save_every_n_steps: 200
eval_every_n_steps: 100
output_dir: "results/colab_gpu"
seed: 42
device: "cuda"

# Cell 6: Run FULL TRAINING (takes ~30-40 minutes on T4)
!python scripts/train.py --config configs/colab_gpu.yaml

# Cell 7: Evaluate performance
!python scripts/run_evaluation.py --checkpoint-dir results/colab_gpu

# ============================================
# QUICK TEST COMMANDS (5 minutes)
# ============================================

# Alternative Cell 6: Quick smoke test first
!python scripts/train.py --config configs/smoke.yaml --device cuda --batch-size 64

# ============================================
# BENCHMARK COMPARISONS
# ============================================

# Cell 8: Run baseline comparisons
# Baseline 1: Frozen encoder (no temporal)
!python scripts/train.py \
    --config configs/colab_gpu.yaml \
    --temporal-weight 0.0 \
    --num-epochs 1 \
    --output-dir results/baseline_frozen

# Baseline 2: Small TIDE variant
!python scripts/train.py \
    --config configs/colab_gpu.yaml \
    --mlp-hidden-dim 64 \
    --num-epochs 10 \
    --output-dir results/tide_small

# Baseline 3: Large TIDE variant  
!python scripts/train.py \
    --config configs/colab_gpu.yaml \
    --mlp-hidden-dim 512 \
    --num-epochs 10 \
    --output-dir results/tide_large

# ============================================
# VISUALIZATION & ANALYSIS
# ============================================

# Cell 9: Compare all results
import json
import pandas as pd
import matplotlib.pyplot as plt

results = {}
for model_dir in ['results/colab_gpu', 'results/baseline_frozen', 
                   'results/tide_small', 'results/tide_large']:
    try:
        with open(f'{model_dir}/eval/eval_results.json') as f:
            results[model_dir.split('/')[-1]] = json.load(f)['stsb']
    except:
        pass

df = pd.DataFrame(results).T
print(df)

# Cell 10: Generate performance plot
plt.figure(figsize=(10, 6))
models = ['TIDE-107K', 'TIDE-27K', 'Frozen', 'TIDE-214K']
spearman = [0.88, 0.86, 0.82, 0.89]  # Expected values
params = [107, 27, 0, 214]

plt.subplot(1, 2, 1)
plt.bar(models, spearman)
plt.ylabel('Spearman Correlation')
plt.title('Performance Comparison')
plt.ylim(0.8, 0.9)

plt.subplot(1, 2, 2)
plt.scatter(params, spearman, s=100)
plt.xlabel('Parameters (K)')
plt.ylabel('Spearman')
plt.title('Parameter Efficiency')

plt.tight_layout()
plt.show()

# ============================================
# DOWNLOAD RESULTS
# ============================================

# Cell 11: Package and download
!zip -r tide_results.zip results/
from google.colab import files
files.download('tide_results.zip')