# DEMRL++ Default Configuration
# Full training configuration for production runs

experiment:
  name: "demrl_plus_full"
  seed: 42
  output_dir: "outputs"
  save_every: 500
  eval_every: 100
  log_every: 10
  use_wandb: false

data:
  sts_b:
    enabled: true
    batch_size: 32
    eval_batch_size: 64
    num_workers: 4
  retrieval:
    enabled: true
    corpus_size: 10000
    query_size: 1000
    batch_size: 32
    eval_batch_size: 128
    index_type: "faiss"  # faiss or exact
    
model:
  encoder:
    name: "sentence-transformers/all-mpnet-base-v2"
    max_length: 128
    pooling: "mean"
  mrl:
    dimensions: [128, 256, 512, 768]
    base_dim: 768
    use_projection: true
    use_layer_norm: true
  consistency:
    enabled: true
    weight: 1.0
    temperature: 0.1
    stop_grad_base: true
    
curriculum:
  enabled: true
  warmup_epochs: 2
  schedule: "exponential"  # linear, exponential, cosine
  initial_bias: 0.6  # bias toward small dims
  final_bias: 0.0   # uniform sampling
  
adapter:
  enabled: true
  method: "lora"
  rank: 8
  alpha: 16
  dropout: 0.1
  cache:
    enabled: true
    capacity: 4
    eviction: "lru"
    few_shot_steps: 10
    few_shot_lr: 1e-4
    
training:
  epochs: 10
  learning_rate: 3e-4
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  fp16: true
  gradient_checkpointing: false
  max_grad_norm: 1.0
  optimizer: "adamw"
  scheduler: "cosine"
  early_stop:
    enabled: true
    patience: 3
    metric: "sts_b_spearman"
    mode: "max"
    
evaluation:
  dimensions_to_test: [128, 256, 512, 768]
  metrics:
    - "spearman"
    - "ndcg@10"
    - "recall@10"
    - "latency"
    - "throughput"
    - "memory"
  baselines:
    - "sentence-transformers/all-MiniLM-L6-v2"
    - "sentence-transformers/all-mpnet-base-v2"
    - "intfloat/e5-base-v2"
    - "BAAI/bge-base-en-v1.5"
    
hardware:
  device: "auto"  # auto, cuda, cpu
  n_gpu: 1
  mixed_precision: "fp16"  # no, fp16, bf16
  
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "outputs/train.log"
