# Training Configuration for Temporal LoRA

# Training hyperparameters
training:
  # Number of epochs per adapter
  num_epochs: 2
  # Batch size per GPU
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  # Gradient accumulation steps (effective batch = batch_size * accum_steps)
  gradient_accumulation_steps: 1
  # Learning rate for Adam optimizer
  learning_rate: 5e-4
  # Weight decay (L2 regularization)
  weight_decay: 0.01
  # Warmup steps (linear warmup)
  warmup_steps: 100
  # Max gradient norm for clipping
  max_grad_norm: 1.0

# Loss function
loss:
  # Contrastive loss type: "cosine", "triplet"
  type: "cosine"
  # Temperature scaling for softmax (contrastive)
  temperature: 0.07
  # Margin for triplet loss (if using triplet)
  margin: 0.5

# Negative sampling
negatives:
  # Number of in-batch negatives
  num_negatives: 8
  # Cross-period-biased negatives (sample from different time buckets)
  cross_period_biased: false
  # Ratio of cross-period negatives (0.0 to 1.0)
  cross_period_ratio: 0.3

# Optimization
optimizer:
  # Optimizer type: "adam", "adamw"
  type: "adamw"
  # Betas for Adam
  betas: [0.9, 0.999]
  # Epsilon for numerical stability
  eps: 1e-8

# Learning rate scheduler
scheduler:
  # Scheduler type: "linear", "cosine", "constant"
  type: "linear"
  # Number of warmup steps
  num_warmup_steps: 100

# Mixed precision training
fp16:
  # Enable FP16 if CUDA available
  enabled: true
  # Automatic mixed precision scaler
  opt_level: "O1"

# Early stopping
early_stopping:
  # Enable early stopping
  enabled: true
  # Patience (epochs without improvement)
  patience: 3
  # Metric to monitor: "val_loss", "val_ndcg"
  metric: "val_loss"
  # Mode: "min" (for loss), "max" (for metrics)
  mode: "min"

# Logging
logging:
  # Log every N steps
  log_steps: 50
  # Evaluation every N steps
  eval_steps: 200
  # Save logs to file
  log_file: "logs/training.log"

# Reproducibility
seed: 42

# Output paths
output:
  model_dir: "models/adapters"
  log_dir: "logs"
  checkpoint_dir: "models/checkpoints"
