# Model Configuration for Temporal LoRA

# Base sentence encoder (frozen)
base_model:
  # Hugging Face model identifier
  name: "sentence-transformers/all-MiniLM-L6-v2"
  # Freeze all base model parameters
  freeze: true
  # Embedding dimension (384 for MiniLM-L6-v2)
  embedding_dim: 384

# LoRA configuration (PEFT)
lora:
  # LoRA rank (controls adapter capacity)
  # Ablation: test 8, 16, 32
  r: 16
  # LoRA alpha (scaling factor)
  lora_alpha: 32
  # Dropout for LoRA layers
  lora_dropout: 0.1
  # Target modules (auto-detect attention Q/K/V)
  # For BERT-based models: ["query", "key", "value"] or ["q_proj", "k_proj", "v_proj"]
  target_modules: ["q_proj", "k_proj", "v_proj"]
  # Bias handling: "none", "all", "lora_only"
  bias: "none"
  # Task type for PEFT
  task_type: "FEATURE_EXTRACTION"

# Per-bucket adapters
adapters:
  # One adapter per time bucket
  # Bucket names must match data.yaml bucket definitions
  buckets: ["pre_2019", "2019_2024"]
  # Save adapters to disk after training
  save_path: "models/adapters"
  # Adapter naming convention: "{bucket_name}_r{rank}"
  naming_template: "{bucket_name}_r{rank}"

# Model checkpointing
checkpointing:
  # Save checkpoints during training
  enabled: true
  # Checkpoint directory
  save_dir: "models/checkpoints"
  # Save every N steps
  save_steps: 500
  # Keep only best N checkpoints (by validation loss)
  save_total_limit: 3
