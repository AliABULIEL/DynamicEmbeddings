paper_id,title,abstract,year
p1,Attention Is All You Need,We propose a new simple network architecture based on attention mechanisms dispensing with recurrence and convolutions entirely. The Transformer model achieves state-of-the-art results on machine translation.,2017
p2,BERT: Pre-training of Deep Bidirectional Transformers,We introduce a new language representation model called BERT which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pretrain deep bidirectional representations.,2018
p3,GPT-2: Language Models are Unsupervised Multitask Learners,We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.,2019
p4,An Image is Worth 16x16 Words: Transformers for Image Recognition,We show that while the Transformer architecture has become the de-facto standard for natural language processing tasks pure transformer applied directly to sequences of image patches can perform very well.,2020
p5,Scaling Laws for Neural Language Models,We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size dataset size and the amount of compute used for training.,2020
p6,GPT-3: Language Models are Few-Shot Learners,We train GPT-3 an autoregressive language model with 175 billion parameters and test its performance in the few-shot setting. For all tasks GPT-3 is applied without any gradient updates or fine-tuning.,2020
p7,CLIP: Learning Transferable Visual Models,We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch.,2021
p8,LoRA: Low-Rank Adaptation of Large Language Models,We propose Low-Rank Adaptation or LoRA which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture.,2021
p9,InstructGPT: Training language models to follow instructions,We make progress on aligning language models with user intent by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API.,2022
p10,Constitutional AI: Harmlessness from AI Feedback,We propose a method for training a harmless AI assistant through self-improvement without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules.,2022
p11,LLaMA: Open and Efficient Foundation Language Models,We introduce LLaMA a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens and show that it is possible to train state-of-the-art models.,2023
p12,GPT-4 Technical Report,We report the development of GPT-4 a large-scale multimodal model which can accept image and text inputs and produce text outputs. GPT-4 exhibits human-level performance on various professional benchmarks.,2023
p13,PaLM 2 Technical Report,We introduce PaLM 2 a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM.,2023
p14,Llama 2: Open Foundation and Fine-Tuned Chat Models,We develop and release Llama 2 a collection of pretrained and fine-tuned large language models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs called Llama 2-Chat are optimized for dialogue use cases.,2023
p15,Mistral 7B,We introduce Mistral 7B a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model and approaches CodeLlama 7B performance on code.,2023
p16,Gemini: A Family of Highly Capable Multimodal Models,We introduce Gemini a family of highly capable multimodal models that can seamlessly understand and combine different types of information including text code audio image and video.,2023
p17,Claude 3 Model Card,We introduce the Claude 3 model family which sets new industry benchmarks across a wide range of cognitive tasks. The family includes Claude 3 Opus Sonnet and Haiku.,2024
p18,Command R: Retrieval Augmented Generation at Production Scale,Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise. Command R boasts high precision on RAG and Tool Use tasks.,2024
p19,Mixtral 8x22B,We present Mixtral 8x22B the latest and most performant model in the Mixtral family. Mixtral 8x22B is a sparse mixture-of-experts model with 39B active parameters and 141B total parameters.,2024
p20,Gemini 1.5: Unlocking multimodal understanding across millions of tokens,We introduce Gemini 1.5 Pro a compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context.,2024
