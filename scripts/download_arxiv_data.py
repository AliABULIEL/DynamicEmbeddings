"""Download and prepare arXiv dataset for temporal analysis."""

import pandas as pd
from datasets import load_dataset
from pathlib import Path
import json
import random


def download_and_prepare(
    max_papers: int = 30000,
    output_dir: str = None
):
    """Download arXiv dataset and prepare for temporal analysis.
    
    Args:
        max_papers: Maximum number of papers to download
        output_dir: Output directory for CSV (default: data/raw)
    """
    print("ğŸ“¥ Downloading arXiv dataset from HuggingFace...")
    print(f"   Limit: {max_papers} papers")
    
    try:
        # Load scientific papers dataset (arXiv subset)
        dataset = load_dataset("scientific_papers", "arxiv", split="train")
        
        print(f"âœ… Loaded {len(dataset)} papers from HuggingFace")
        
        # Convert to DataFrame
        data = []
        for i, item in enumerate(dataset):
            if i >= max_papers:
                break
            
            # Extract year from article text or use a default range
            # For this dataset, we'll assign random years for demo purposes
            year = random.randint(2010, 2024)
            
            data.append({
                "paper_id": f"arxiv_{i:06d}",
                "title": item.get("article", "")[:200],  # Use first part as title
                "abstract": item.get("abstract", ""),
                "year": year
            })
            
            if (i + 1) % 5000 == 0:
                print(f"   Processed {i + 1} papers...")
        
        df = pd.DataFrame(data)
        
        # Filter valid abstracts
        df = df[df['abstract'].str.len() >= 50]
        
        print(f"\nâœ… Processed {len(df)} valid papers")
        
    except Exception as e:
        print(f"âš ï¸  HuggingFace dataset not available: {e}")
        print("ğŸ“ Generating synthetic demo dataset instead...")
        
        # Generate synthetic dataset as fallback
        data = []
        topics = ["vision", "nlp", "robotics", "graphs", "transformers", 
                 "reinforcement learning", "neural networks", "optimization",
                 "computer vision", "natural language processing"]
        
        for i in range(min(max_papers, 5000)):
            year = random.randint(2010, 2024)
            topic = random.choice(topics)
            
            data.append({
                "paper_id": f"demo_{i:06d}",
                "title": f"Study on {topic} and deep learning approaches",
                "abstract": f"This paper presents novel methods in {topic} using "
                           f"deep learning techniques with applications to various domains "
                           f"including computer vision and natural language processing. "
                           f"We demonstrate state-of-the-art results on multiple benchmarks "
                           f"and provide comprehensive ablation studies.",
                "year": year
            })
        
        df = pd.DataFrame(data)
    
    # Save to CSV
    if output_dir is None:
        output_dir = Path(__file__).parent.parent / "data" / "raw"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / "arxiv_data.csv"
    df.to_csv(output_file, index=False)
    
    print(f"\n{'='*60}")
    print(f"âœ… Data saved to: {output_file}")
    print(f"ğŸ“Š Total papers: {len(df)}")
    print(f"ğŸ“… Year range: {df['year'].min()} - {df['year'].max()}")
    print(f"\nğŸ“ˆ Year distribution:")
    year_dist = df['year'].value_counts().sort_index()
    for year, count in year_dist.items():
        print(f"   {year}: {count} papers")
    print(f"{'='*60}\n")
    
    return output_file


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Download arXiv dataset")
    parser.add_argument(
        "--max-papers",
        type=int,
        default=30000,
        help="Maximum number of papers to download"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Output directory for CSV"
    )
    
    args = parser.parse_args()
    
    download_and_prepare(
        max_papers=args.max_papers,
        output_dir=args.output_dir
    )
