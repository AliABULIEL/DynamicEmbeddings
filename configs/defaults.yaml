# TIDE-Lite Default Configuration
# This file documents every configuration parameter used in the system.
# All values here are defaults that work for most scenarios.

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

# Base encoder model from HuggingFace
model_name: "sentence-transformers/all-MiniLM-L6-v2"

# Dimension of encoder hidden states
hidden_dim: 384

# Dimension for temporal position encoding (must be even for sinusoidal)
time_dims: 32

# Hidden dimension of temporal MLP
time_mlp_hidden: 128

# Dropout rate in temporal MLP (0.0 to 1.0)
mlp_dropout: 0.1

# Whether to freeze base encoder weights during training
freeze_encoder: true

# Pooling strategy for sentence embeddings: "mean", "cls", or "max"
pooling_strategy: "mean"

# Activation for temporal gating: "sigmoid" or "tanh"
gate_activation: "sigmoid"

# ============================================================================
# DATA CONFIGURATION
# ============================================================================

# Maximum sequence length in tokens
max_seq_len: 128

# Training batch size
batch_size: 32

# Evaluation batch size (can be larger than training)
eval_batch_size: 64

# Number of dataloader workers
num_workers: 2

# Cache directory for datasets
cache_dir: "./data"

# Directory for TimeQA dataset (if using real temporal data)
timeqa_data_dir: "./data/timeqa"

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================

# Number of training epochs
epochs: 10

# Peak learning rate
lr: 3e-5

# Number of warmup steps
warmup_steps: 500

# Weight decay for AdamW optimizer
weight_decay: 0.01

# Gradient clipping value
gradient_clip: 1.0

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================

# Weight for temporal consistency loss
consistency_weight: 0.01

# Weight for base embedding preservation loss
preservation_weight: 0.005

# Multi-task learning ratio (STS-B:Temporal batches)
# 1 means 1:1 alternation, 2 means 2:1 (2 STS-B per 1 temporal)
mtl_ratio: 1

# Lambda for temporal loss weight in multi-task learning
# This replaces consistency_weight when using separate losses
lambda_temporal: 0.1

# Time constant for temporal loss (seconds) - default 1 day
tau_seconds: 86400.0

# ============================================================================
# HARDWARE & OPTIMIZATION
# ============================================================================

# Device: "cuda", "cpu", or null (auto-detect)
device: null

# Enable automatic mixed precision (AMP) for faster training
use_amp: true

# Random seed for reproducibility
seed: 42

# ============================================================================
# CHECKPOINTING & EVALUATION
# ============================================================================

# Evaluate model every N steps
eval_every: 100

# Save checkpoint every N steps
save_every: 500

# Output directory for all results
out_dir: "results/default"

# Checkpoint subdirectory (if null, uses out_dir/checkpoints)
checkpoint_dir: null

# ============================================================================
# RETRIEVAL & INDEXING
# ============================================================================

# FAISS index type for retrieval tasks
# Options: "FlatIP" (exact), "IVF" (approximate), "HNSW" (graph-based)
faiss: "FlatIP"

# Number of clusters for IVF index (if using)
faiss_n_clusters: 100

# Number of probes for IVF search
faiss_n_probe: 10

# ============================================================================
# LOGGING & DEBUGGING
# ============================================================================

# Logging level: "DEBUG", "INFO", "WARNING", "ERROR"
log_level: "INFO"

# Dry run mode - prints plan without execution
dry_run: false

# Enable temporal components (set false for ablation)
temporal_enabled: true

# ============================================================================
# EVALUATION TASKS
# ============================================================================

# Which evaluation tasks to run
eval_tasks:
  - "stsb"      # Semantic Textual Similarity Benchmark
  - "quora"     # Quora Question Pairs (retrieval)
  - "temporal"  # Temporal evaluation (TimeQA/TempLAMA)

# Evaluation-specific settings
eval_stsb_split: "test"
eval_quora_split: "test"
eval_temporal_split: "test"

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================

# Enable gradient accumulation (effective_batch = batch_size * accum_steps)
gradient_accumulation_steps: 1

# Early stopping patience (0 = disabled)
early_stopping_patience: 0

# Learning rate scheduler: "cosine", "linear", or "constant"
lr_scheduler: "cosine"

# Exponential moving average of model weights (0 = disabled)
ema_decay: 0.0

# ============================================================================
# DATASET PATHS (for reproducibility)
# ============================================================================

# Paths to specific dataset versions
stsb_path: "nyu-mll/glue"
stsb_config: "stsb"
quora_path: "quora"
quora_config: null

# Temporal dataset selection: "timeqa" or "templama"
temporal_dataset: "templama"  # TempLAMA as default (easier to download)

# Path to TempLAMA dataset files
# Download from: https://github.com/google-research/language/tree/master/language/templama
templama_path: "./data/templama"

# Skip temporal evaluation if datasets not available
# Can be overridden with --skip-temporal CLI flag
skip_temporal: false
