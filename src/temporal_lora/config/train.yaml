# Training configuration
training:
  epochs: 2
  batch_size: 32
  learning_rate: 2.0e-5
  
  # Optimization
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0  # Gradient clipping
  
  # Mixed precision
  fp16: true  # Enable when CUDA available
  
  # Gradient accumulation
  gradient_accumulation_steps: 1

dataloader:
  num_workers: 2
  pin_memory: true
  shuffle: true

negatives:
  # Negative sampling strategy
  cross_period_negatives: true  # Sample hard negatives from other time buckets
  num_negatives: 3  # Negatives per positive
  in_batch_negatives: true  # Use other batch samples as negatives

loss:
  type: "contrastive"  # InfoNCE / contrastive loss
  temperature: 0.05

validation:
  eval_steps: 500  # Evaluate every N steps
  eval_split: 0.1  # Hold out 10% for validation
  early_stopping_patience: 3
  metric: "loss"  # Monitor validation loss

checkpointing:
  save_steps: 1000
  save_total_limit: 2  # Keep only best 2 checkpoints
  save_strategy: "steps"

logging:
  log_level: "INFO"
  log_steps: 100

seed: 42
