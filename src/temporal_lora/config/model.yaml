# Model configuration
base_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  freeze: true  # Keep base model frozen
  max_seq_length: 256

lora:
  # LoRA hyperparameters
  r: 16  # Rank
  lora_alpha: 32  # Scaling factor
  lora_dropout: 0.1
  
  # Target modules (attention Q/K/V)
  target_modules:
    - "query"
    - "key"
    - "value"
  
  # PEFT config
  bias: "none"
  task_type: "FEATURE_EXTRACTION"
  
  # Inference mode
  inference_mode: false

pooling:
  strategy: "mean"  # Mean pooling over token embeddings
